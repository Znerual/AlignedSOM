{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe7dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "from minisom import MiniSom\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randrange\n",
    "\n",
    "from data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07583a17",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "Aligned SOMs aims at training mulitple layers of n SOMs with differently weighted subsets of attributes.\n",
    "\n",
    "The Alignd SOM implementation extends the well known MiniSom package.\n",
    "\n",
    "\n",
    "## Layer Weighting\n",
    "\n",
    "Two aspects or concepts of features in a dataset are differently weightet by different layers of the Alignd SOMs. The first layer uses a weighting ratio between aspect A and aspect B features of 1:0. The middle or center layer, weights both aspects equally. The last layer uses a weighting ratio of 0:1.\n",
    "\n",
    "We create the weights by layer in te **AlignedSom** class using the method **_create_weights_by_layer**. The **AlignedSom** accepts a parameter **concept_indices** which has to be a boolean List inidcating if the feature belongs to aspect A (True or 1) or if it belongs to aspect B (False or 0). \n",
    "\n",
    "## Layer inizialization (Orientation / Codebook inizialization)\n",
    "\n",
    "We create n SOM layers inizializing them  identically using the same common codebook but weighting them by the respective layer weight vector (from 0 for group A attributes and 1 for group B to 0/a for groups A and B in n steps. \n",
    "\n",
    "The inizialization of the layers in the **AlignedSom** class is done in the method **_create_layers**. We either crate the common codebook randomly or train the center SOM (trained with unweighted data) and use it as basis for all layer inizializations. This can be changeed by the parameter **initial_codebook_inizialization** (\"random\" or \"pretrained).\n",
    "The weighting of the layers is done by the **weights_by_layer** as explaind in the previous section. One Layer is represented by the **Layer** class which extends the MiniSom algorithm by overwriting the **update** method.\n",
    "\n",
    "## Training\n",
    "\n",
    "We train multiple layers of SOMs iteratively with an online training algorithm.\n",
    "1) select a random layer and a random observation from the dataset\n",
    "2) select the winning unit in the selected layer based on the weighted feature vector\n",
    "3) train all layers updating the weights based on the same winning unint\n",
    "    * the randomly selected layer is updated as in the normal SOM training\n",
    "    * all other layers update the weights similarly but the margin of the update is based on the distance to the selected layer\n",
    "    * all layers use the weighted feature vector based on their respective layer weights\n",
    "4) iterate steps 1-3 N times\n",
    "\n",
    "### Layer distances\n",
    "\n",
    "The distance of the layers is defined as follows.\n",
    "* the distance to the layer to iteslf is 1.0 -> normal SOM update rule\n",
    "* the distance to the neighboring layer is a fraction (layer_distance_ratio) of the distance between neighbooring units in one layer (by default 1/10)\n",
    "* the distance between the first and the last layer is the same distance as the distance of the upper left and lower right unit of each map.\n",
    "* the distance between all other layers is a linear interploation of the previus two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e91808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from minisom import MiniSom\n",
    "\n",
    "\n",
    "class Layer(MiniSom):\n",
    "    def __init__(self,\n",
    "                 dimension: Tuple[int, int],\n",
    "                 input_len,\n",
    "                 initial_codebook,\n",
    "                 sigma=1,\n",
    "                 learning_rate=0.5,\n",
    "                 neighborhood_function='gaussian',\n",
    "                 activation_distance='euclidean',\n",
    "                 random_seed=None):\n",
    "        super().__init__(\n",
    "            x=dimension[0],\n",
    "            y=dimension[1],\n",
    "            input_len=input_len,\n",
    "            sigma=sigma,\n",
    "            learning_rate=learning_rate,\n",
    "            neighborhood_function=neighborhood_function,\n",
    "            topology='rectangular',\n",
    "            activation_distance=activation_distance,\n",
    "            random_seed=random_seed)\n",
    "\n",
    "        self._weights = initial_codebook\n",
    "\n",
    "    # changed update to include the distance to the layer in the neighborhood\n",
    "    # todo: not sure if only winning unit updated or whole neighbourhood for other layers\n",
    "    def update(self, x, win, layer_dist, t, max_iteration):\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig) * eta * layer_dist\n",
    "        # w_new = eta * neighborhood_function * (x-w)\n",
    "        self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eacb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignedSom(MiniSom):\n",
    "    def __init__(self,\n",
    "                 dimension: Tuple[int, int],\n",
    "                 data: np.ndarray,  # 2d numpy array\n",
    "                 concept_indices: List[bool],  # boolean list if feature belongs to concept A or concept B\n",
    "                 num_layers: int = 100,\n",
    "                 layer_distance_ratio: float = 0.1,\n",
    "                 sigma: float = 1.0,\n",
    "                 learning_rate: float = 0.5,\n",
    "                 neighborhood_function: str = 'gaussian',\n",
    "                 activation_distance: str = 'euclidean',\n",
    "                 initial_codebook_inizialization: str = 'random',  # random or pretrained\n",
    "                 random_seed=None):\n",
    "        super().__init__(\n",
    "            x=dimension[0],\n",
    "            y=dimension[1],\n",
    "            input_len=data.shape[1],\n",
    "            sigma=sigma,\n",
    "            learning_rate=learning_rate,\n",
    "            neighborhood_function=neighborhood_function,\n",
    "            topology='rectangular',\n",
    "            activation_distance=activation_distance,\n",
    "            random_seed=random_seed)\n",
    "        self.data = data\n",
    "        self.dimension = dimension\n",
    "        self.concept_indices = concept_indices\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_distance_ratio = layer_distance_ratio\n",
    "        self._neighborhood_function = neighborhood_function\n",
    "        self._initial_codebook_inizialization = initial_codebook_inizialization\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.weights_by_layer: np.ndarray = self._create_weights_by_layer()\n",
    "        self.layers: List[Layer] = self._create_layers()\n",
    "        self.layer_distances = self._create_layer_distances()\n",
    "\n",
    "    def train(self,\n",
    "              data: np.ndarray,  # 2d numpy array,\n",
    "              num_iterations):\n",
    "        n_observations = data.shape[0]\n",
    "        for t in tqdm(range(num_iterations)):\n",
    "            selected_layer = randrange(0, self.num_layers)\n",
    "            selected_observation = randrange(0, n_observations)\n",
    "            # print(f'selected layer: {selected_layer}')\n",
    "            # print(f'selected observation: {selected_observation}')\n",
    "            winner = self.layers[selected_layer].winner(\n",
    "                data[selected_observation] * self.weights_by_layer[selected_layer])\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                # print(f'current layer: {i}')\n",
    "                # ĺayer_dist = self.layer_distance(t, num_iterations, np.abs(selected_layer - i))\n",
    "                ĺayer_dist = self.layer_distances[np.abs(selected_layer - i)]\n",
    "                # print(f'distance: {ĺayer_dist}')\n",
    "                layer.update(data[selected_observation] * self.weights_by_layer[i],\n",
    "                             winner,\n",
    "                             ĺayer_dist,\n",
    "                             t,\n",
    "                             num_iterations)\n",
    "\n",
    "    # the distance between one layer and the next is defined by the distance between neighboring units\n",
    "    # multiplyed by some fraction \"layer_distance_ratio\"\n",
    "    def layer_distance(self, t, max_iteration, grid_distance):\n",
    "        if grid_distance == 0.0:\n",
    "            return 1.0\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        distance_neighboring_units = self.neighborhood((0, 0), sig)[(0, 1)]\n",
    "        return distance_neighboring_units * (self.layer_distance_ratio / grid_distance)\n",
    "\n",
    "    # return the codebook weights for all layers\n",
    "    def get_layer_weights(self) -> List[np.ndarray]:\n",
    "        return [layer.get_weights() for layer in self.layers]\n",
    "    \n",
    "    def _create_layer_distances(self):\n",
    "        # used default gaussian with sigma = 1.0 for distance between layers\n",
    "        x_mash, y_mash = np.meshgrid(np.arange(1), np.arange(self.num_layers - 1))\n",
    "        d = 2\n",
    "        ax = np.exp(-np.power(x_mash-x_mash[0], 2)/d)\n",
    "        ay = np.exp(-np.power(y_mash-y_mash[0], 2)/d)\n",
    "        layer_distances = (ax * ay).T[0]\n",
    "        layer_distances *= self.layer_distance_ratio\n",
    "        layer_distances = np.insert(layer_distances, 0, 1.0)  # distence to layer itself\n",
    "        return layer_distances\n",
    "    \n",
    "    def _gaussian_for_layer(c, sigma):\n",
    "        d = 2\n",
    "        ax = np.exp(-np.power(aa-aa[c], 2)/d)\n",
    "        ay = np.exp(-np.power(bb-bb[c], 2)/d)\n",
    "        return (ax * ay).T  # the external product gives a matrix    \n",
    "\n",
    "    # create a weights matrix for two concepts in a feature matrix\n",
    "    # the shape corresponds to shape (num_layers, input_len))\n",
    "    # where num_soms is the number of soms trained\n",
    "    def _create_weights_by_layer(self):\n",
    "        if self.concept_indices.shape[0] != self._input_len:\n",
    "            raise AttributeError('concept_indices has to have the same dimension as input_len')\n",
    "        column_weights = []\n",
    "        weights_concept_1 = np.linspace(0, 1, self.num_layers)\n",
    "        weights_concept_2 = np.linspace(1, 0, self.num_layers)\n",
    "        for i in self.concept_indices:\n",
    "            if i:\n",
    "                column_weights.append(weights_concept_1)\n",
    "            else:\n",
    "                column_weights.append(weights_concept_2)\n",
    "        return np.column_stack(column_weights)\n",
    "\n",
    "    # initialize all layers of the aligned SOM\n",
    "    def _create_layers(self) -> List[Layer]:\n",
    "        layers = []\n",
    "        if self._initial_codebook_inizialization == 'random':\n",
    "            inital_weights = self._create_random_weights()\n",
    "        elif self._initial_codebook_inizialization == 'pretrained':\n",
    "            inital_weights = self._create_weights_by_training_one_some()\n",
    "        else:\n",
    "            raise AttributeError('initial_codebook_inizialization has to be \"random\" or \"pretrained\"')\n",
    "        for weights in self.weights_by_layer:\n",
    "            layers.append(Layer(\n",
    "                dimension=self.dimension,\n",
    "                input_len=self._input_len,\n",
    "                initial_codebook=np.array(inital_weights * weights, dtype=np.float32),\n",
    "                sigma=self._sigma,\n",
    "                learning_rate=self._learning_rate,\n",
    "                neighborhood_function=self._neighborhood_function,\n",
    "                activation_distance=self._activation_distance,\n",
    "                random_seed=self.random_seed))\n",
    "        return layers\n",
    "\n",
    "    def _create_random_weights(self):\n",
    "        if self.random_seed:\n",
    "            np.random.seed(self.random_seed)\n",
    "        return np.random.random((self.dimension[0], self.dimension[1], self._input_len))\n",
    "\n",
    "    def _create_weights_by_training_one_some(self):\n",
    "        # som trained on not weighted features (same as middle layer)\n",
    "        middle_som = MiniSom(\n",
    "            x=self.dimension[0],\n",
    "            y=self.dimension[1],\n",
    "            input_len=self._input_len,\n",
    "            sigma=self._sigma,\n",
    "            learning_rate=self._learning_rate,\n",
    "            neighborhood_function=self._neighborhood_function,\n",
    "            topology='rectangular',\n",
    "            activation_distance=self._activation_distance,\n",
    "            random_seed=self.random_seed)\n",
    "        middle_som.train(self.data, 1000)\n",
    "        return middle_som.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c21baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HitHistogram\n",
    "def HitHist(_m, _n, _weights, _idata, upscaling_factor=1000):\n",
    "    hist = np.zeros(_m * _n)\n",
    "    for vector in _idata: \n",
    "        position =np.argmin(np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1)))\n",
    "        hist[position] += 1\n",
    "    \n",
    "    hist_reshaped = hist.reshape(_m, _n)\n",
    "    return resize(hist_reshaped, (upscaling_factor, upscaling_factor), mode='constant')\n",
    "\n",
    "#U-Matrix - implementation\n",
    "def UMatrix(_m, _n, _weights, _dim, upscaling_factor=1000):\n",
    "    U = _weights.reshape(_m, _n, _dim)\n",
    "    U = np.insert(U, np.arange(1, _n), values=0, axis=1)\n",
    "    U = np.insert(U, np.arange(1, _m), values=0, axis=0)\n",
    "    #calculate interpolation\n",
    "    for i in range(U.shape[0]): \n",
    "        if i%2==0:\n",
    "            for j in range(1,U.shape[1],2):\n",
    "                U[i,j][0] = np.linalg.norm(U[i,j-1] - U[i,j+1], axis=-1)\n",
    "        else:\n",
    "            for j in range(U.shape[1]):\n",
    "                if j%2==0: \n",
    "                    U[i,j][0] = np.linalg.norm(U[i-1,j] - U[i+1,j], axis=-1)\n",
    "                else:      \n",
    "                    U[i,j][0] = (np.linalg.norm(U[i-1,j-1] - U[i+1,j+1], axis=-1) + np.linalg.norm(U[i+1,j-1] - U[i-1,j+1], axis=-1))/(2*np.sqrt(2))\n",
    "\n",
    "    U = np.sum(U, axis=2) #move from Vector to Scalar\n",
    "\n",
    "    for i in range(0, U.shape[0], 2): #count new values\n",
    "        for j in range(0, U.shape[1], 2):\n",
    "            region = []\n",
    "            if j>0: region.append(U[i][j-1]) #check left border\n",
    "            if i>0: region.append(U[i-1][j]) #check bottom\n",
    "            if j<U.shape[1]-1: region.append(U[i][j+1]) #check right border\n",
    "            if i<U.shape[0]-1: region.append(U[i+1][j]) #check upper border\n",
    "\n",
    "            U[i,j] = np.median(region)\n",
    "    return resize(U, (upscaling_factor, upscaling_factor), mode='constant')\n",
    "    #return U\n",
    "\n",
    "\n",
    "#SDH - implementation\n",
    "def SDH(_m, _n, _weights, _idata, factor=2, approach=0, upscaling_factor=1000):\n",
    "    import heapq\n",
    "\n",
    "    sdh_m = np.zeros( _m * _n)\n",
    "\n",
    "    cs=0\n",
    "    for i in range(factor): cs += factor-i\n",
    "\n",
    "    for vector in _idata:\n",
    "        dist = np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1))\n",
    "        c = heapq.nsmallest(factor, range(len(dist)), key=dist.__getitem__)\n",
    "        if (approach==0): # normalized\n",
    "            for j in range(factor):  sdh_m[c[j]] += (factor-j)/cs \n",
    "        if (approach==1):# based on distance\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0/dist[c[j]] \n",
    "        if (approach==2): \n",
    "            dmin, dmax = min(dist[c]), max(dist[c])\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0 - (dist[c[j]]-dmin)/(dmax-dmin)\n",
    "    \n",
    "    sdh_m = sdh_m.reshape(_m, _n)\n",
    "    return resize(sdh_m, (upscaling_factor, upscaling_factor), mode='constant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6908a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aligned_som(asom: AlignedSom, data:np.ndarray, visualization_function=SDH, num_plots=5, value_range=(0,5), **kwargs):\n",
    "    \"\"\"Plot the aligned SOM\n",
    "\n",
    "    Args:\n",
    "        asom (AlignedSom): trained aligned SOM to plot\n",
    "        data (np.ndarray): input data to use for the visualization\n",
    "        visualization_function (Callable, optional): Which visualization to use. Options are: SDH, HitHist and UMatrix. Defaults to SDH.\n",
    "        num_plots (int, optional): How many intermediary plots to show. Defaults to 5.\n",
    "        value_range (tuple, optional): Value range of the histogram given as tuple of min and max values. Defaults to (0,5).\n",
    "        kwargs: Additional arguments to pass to the visualization function\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib figure: Figure object_\n",
    "    \"\"\"\n",
    "    assert num_plots <= asom.num_layers, \"Number of plots must be less than or equal to the number of layers\"\n",
    "    \n",
    "    # calculate the histograms\n",
    "    visualizations = []\n",
    "    for layer_weights in asom.get_layer_weights():\n",
    "        layer_weights = np.reshape(layer_weights, (asom.dimension[0] * asom.dimension[1], data.shape[1]))\n",
    "        if visualization_function == UMatrix:\n",
    "            histogram = visualization_function(asom.dimension[0], asom.dimension[1], layer_weights, data.shape[1], **kwargs)\n",
    "        else:\n",
    "            histogram = visualization_function(asom.dimension[0], asom.dimension[1], layer_weights, data, **kwargs)\n",
    "        visualizations.append(histogram)\n",
    "    \n",
    "    # decrease figure size to increase plotting speed for larger plots\n",
    "    if num_plots > 32:\n",
    "        figsize = (0.75*num_plots, 0.6125)\n",
    "    if num_plots > 16:\n",
    "        figsize = (1.5*num_plots, 1.25)\n",
    "    elif num_plots > 8:\n",
    "        figsize = (3*num_plots, 2.5)\n",
    "    else:\n",
    "        figsize=(6*num_plots,5)\n",
    "    \n",
    "    # create the plot\n",
    "    figure, axis = plt.subplots(1, num_plots, figsize=figsize)\n",
    "    for i, vis_i in enumerate(np.linspace(0, asom.num_layers - 1, num_plots, dtype=int)):\n",
    "        hp = sns.heatmap(visualizations[vis_i], ax=axis[i], vmin=value_range[0], vmax=value_range[1], cbar=False, cmap='viridis')\n",
    "        hp.set(xticklabels=[])\n",
    "        hp.set(yticklabels=[])\n",
    "        axis[i].tick_params(left=False, bottom=False)\n",
    "    plt.show()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a6db4-fca6-4b63-8c04-e916134c8534",
   "metadata": {},
   "source": [
    "# Setup Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25cf848",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6e056-81ff-49c9-bf56-70d670f10559",
   "metadata": {},
   "source": [
    "# Chainlink 100x60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1eb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_100_60(N_LAYERS=31, sigma=1.0, concept_indices=[1, 1, 0], layer_distance_ratio=0.1, learning_rate=0.5, num_plots=5):\n",
    "    SEED = 12345\n",
    "    SOM_DIM = (100, 60)\n",
    "\n",
    "    input_data, components, weights, classinfo = load_dataset('chainlink')\n",
    "    data = input_data['arr']\n",
    "    concept_indices = np.array(concept_indices)\n",
    "   \n",
    "    asom = AlignedSom(\n",
    "        SOM_DIM, data, concept_indices,\n",
    "        num_layers=N_LAYERS,\n",
    "        sigma=sigma,\n",
    "        learning_rate=learning_rate,\n",
    "        layer_distance_ratio=layer_distance_ratio,\n",
    "        initial_codebook_inizialization='random')\n",
    "    \n",
    "    asom.train(data, TRAIN_STEPS * N_LAYERS)\n",
    "    np.save(f\"../results/chainlink_setup_100_60_{N_LAYERS}_{sigma}_{layer_distance_ratio}_{learning_rate}_{'-'.join([str(c) for c in concept_indices])}.npy\", np.array(asom.get_layer_weights()))\n",
    "    \n",
    "    fig = plot_aligned_som(asom, data, num_plots=num_plots)\n",
    "    fig.savefig(f\"../results/chainlink_setup_100_60_{N_LAYERS}_{sigma}_{layer_distance_ratio}_{learning_rate}_{'-'.join([str(c) for c in concept_indices])}.pdf\")\n",
    "    \n",
    "    return data, asom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74f0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 307/31000 [00:08<14:40, 34.84it/s]"
     ]
    }
   ],
   "source": [
    "data_large_1, asom_large_1 = setup_100_60()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large_2, asom_large_2 = setup_100_60(N_LAYERS=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large_3, asom_large_3 = setup_100_60(layer_distance_ratio=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca023ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large_4, asom_large_4 = setup_100_60(layer_distance_ratio=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bfa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large_5, asom_large_5 = setup_100_60(N_LAYERS=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214427be-c555-4ff6-9726-85b9842b08de",
   "metadata": {},
   "source": [
    "# Chainlink 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_10_10(N_LAYERS=31, sigma=1.0, concept_indices=[1, 1, 0], layer_distance_ratio=0.1, learning_rate=0.5, num_plots=5):\n",
    "    SEED = 12345\n",
    "    SOM_DIM = (10, 10)\n",
    "\n",
    "    input_data, components, weights, classinfo = load_dataset('chainlink')\n",
    "    data = input_data['arr']\n",
    "    concept_indices = np.array(concept_indices)\n",
    "   \n",
    "    asom = AlignedSom(\n",
    "        SOM_DIM, data, concept_indices,\n",
    "        num_layers=N_LAYERS,\n",
    "        sigma=sigma,\n",
    "        learning_rate=learning_rate,\n",
    "        layer_distance_ratio=layer_distance_ratio,\n",
    "        initial_codebook_inizialization='random')\n",
    "    \n",
    "    asom.train(data, TRAIN_STEPS * N_LAYERS)\n",
    "    np.save(f\"../results/chainlink_setup_10_10_{N_LAYERS}_{sigma}_{layer_distance_ratio}_{learning_rate}_{'-'.join([str(c) for c in concept_indices])}.npy\", np.array(asom.get_layer_weights()))\n",
    "    \n",
    "    fig = plot_aligned_som(asom, data, num_plots=num_plots)\n",
    "    fig.savefig(f\"../results/chainlink_setup_10_10_{N_LAYERS}_{sigma}_{layer_distance_ratio}_{learning_rate}_{'-'.join([str(c) for c in concept_indices])}.pdf\")\n",
    "    \n",
    "    return data, asom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e507f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default Setup\n",
    "data_small_1, asom_small_1 = setup_10_10()\n",
    "## Many In-Between Layers\n",
    "data_small_2, asom_small_2 = setup_10_10(N_LAYERS=128)\n",
    "## Strong Layer-Wise Coupling\n",
    "data_small_3, asom_small_3 = setup_10_10(layer_distance_ratio=10)\n",
    "## Weak Layer-Wise Coupling\n",
    "data_small_4, asom_small_4 = setup_10_10(layer_distance_ratio=1e-3)\n",
    "## Few In-Between Layers\n",
    "data_small_5, asom_small_5 = setup_10_10(N_LAYERS=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45d4bc",
   "metadata": {},
   "source": [
    "Vary learnign rate, sigma, which features are attribute A and which B number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2a9d8-3e87-4e9e-a5ae-dafd481f1fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87e40a-ea19-423b-8830-33ddac89ae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfd0c7e95e2101a5731fc22994b2564359838c2d832b63cc38588ffdd51c5ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
