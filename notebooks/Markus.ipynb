{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PySOMVis as PySomVisModule\n",
    "from PySOMVis.pysomvis import PySOMVis\n",
    "from PySOMVis.SOMToolBox_Parse import SOMToolBox_Parse\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "from minisom import MiniSom\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import somoclu\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "# import panel as pn\n",
    "\n",
    "\n",
    "# import holoviews as hv\n",
    "# hv.extension('bokeh')\n",
    "#hv.extension('bokeh')\n",
    "# import os\n",
    "\n",
    "DATASET_PATH = Path(PySomVisModule.__file__).parent / 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c225ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1)\n",
    "b = np.arange(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73992567",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa, bb = np.meshgrid(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de521e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(c, sigma):\n",
    "    d = 2*sigma*sigma\n",
    "    ax = np.exp(-np.power(aa-aa[c], 2)/d)\n",
    "    ay = np.exp(-np.power(bb-bb[c], 2)/d)\n",
    "    return (ax * ay).T  # the external product gives a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d31559",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = (gaussian(0, 1.) / 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27905162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07583a17",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "Aligned SOMs aims at training mulitple layers of n SOMs with differently weighted subsets of attributes.\n",
    "\n",
    "The Alignd SOM implementation extends the well known MiniSom package.\n",
    "\n",
    "\n",
    "## Layer Weighting\n",
    "\n",
    "Two aspects or concepts of features in a dataset are differently weightet by different layers of the Alignd SOMs. The first layer uses a weighting ratio between aspect A and aspect B features of 1:0. The middle or center layer, weights both aspects equally. The last layer uses a weighting ratio of 0:1.\n",
    "\n",
    "We create the weights by layer in te **AlignedSom** class using the method **_create_weights_by_layer**. The **AlignedSom** accepts a parameter **concept_indices** which has to be a boolean List inidcating if the feature belongs to aspect A (True or 1) or if it belongs to aspect B (False or 0). \n",
    "\n",
    "## Layer inizialization (Orientation / Codebook inizialization)\n",
    "\n",
    "We create n SOM layers inizializing them  identically using the same common codebook but weighting them by the respective layer weight vector (from 0 for group A attributes and 1 for group B to 0/a for groups A and B in n steps. \n",
    "\n",
    "The inizialization of the layers in the **AlignedSom** class is done in the method **_create_layers**. We either crate the common codebook randomly or train the center SOM (trained with unweighted data) and use it as basis for all layer inizializations. This can be changeed by the parameter **initial_codebook_inizialization** (\"random\" or \"pretrained).\n",
    "The weighting of the layers is done by the **weights_by_layer** as explaind in the previous section. One Layer is represented by the **Layer** class which extends the MiniSom algorithm by overwriting the **update** method.\n",
    "\n",
    "## Training\n",
    "\n",
    "We train multiple layers of SOMs iteratively with an online training algorithm.\n",
    "1) select a random layer and a random observation from the dataset\n",
    "2) select the winning unit in the selected layer based on the weighted feature vector\n",
    "3) train all layers updating the weights based on the same winning unint\n",
    "    * the randomly selected layer is updated as in the normal SOM training\n",
    "    * all other layers update the weights similarly but the margin of the update is based on the distance to the selected layer\n",
    "    * all layers use the weighted feature vector based on their respective layer weights\n",
    "4) iterate steps 1-3 N times\n",
    "\n",
    "### Layer distances\n",
    "\n",
    "The distance of the layers is defined as follows.\n",
    "* the distance to the layer to iteslf is 1.0 -> normal SOM update rule\n",
    "* the distance to the neighboring layer is a fraction (layer_distance_ratio) of the distance between neighbooring units in one layer (by default 1/10)\n",
    "* the distance between the first and the last layer is the same distance as the distance of the upper left and lower right unit of each map.\n",
    "* the distance between all other layers is a linear interploation of the previus two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_file_string(dataset_name: str, extension: str) -> str:\n",
    "    return str((DATASET_PATH / dataset_name / f'{dataset_name}.{extension}').resolve())\n",
    "\n",
    "# load one of animals, iris, chainlink, 10clusters, BostonHousing\n",
    "def load_dataset(name: str):\n",
    "    input_data = SOMToolBox_Parse(db_file_string(name, 'vec')).read_weight_file()\n",
    "    components = SOMToolBox_Parse(db_file_string(name, 'tv')).read_weight_file()\n",
    "    weights = SOMToolBox_Parse(db_file_string(name, 'wgt.gz')).read_weight_file()\n",
    "    classinfo = SOMToolBox_Parse(db_file_string(name, 'cls')).read_weight_file()\n",
    "    return input_data, components, weights, classinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e91808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from minisom import MiniSom\n",
    "\n",
    "\n",
    "class Layer(MiniSom):\n",
    "    def __init__(self,\n",
    "                 dimension: Tuple[int, int],\n",
    "                 input_len,\n",
    "                 initial_codebook,\n",
    "                 sigma=1,\n",
    "                 learning_rate=0.5,\n",
    "                 neighborhood_function='gaussian',\n",
    "                 activation_distance='euclidean',\n",
    "                 random_seed=None):\n",
    "        super().__init__(\n",
    "            x=dimension[0],\n",
    "            y=dimension[1],\n",
    "            input_len=input_len,\n",
    "            sigma=sigma,\n",
    "            learning_rate=learning_rate,\n",
    "            neighborhood_function=neighborhood_function,\n",
    "            topology='rectangular',\n",
    "            activation_distance=activation_distance,\n",
    "            random_seed=random_seed)\n",
    "\n",
    "        self._weights = initial_codebook\n",
    "\n",
    "    # changed update to include the distance to the layer in the neighborhood\n",
    "    # todo: not sure if only winning unit updated or whole neighbourhood for other layers\n",
    "    def update(self, x, win, layer_dist, t, max_iteration):\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig) * eta * layer_dist\n",
    "        # w_new = eta * neighborhood_function * (x-w)\n",
    "        self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignedSom(MiniSom):\n",
    "    def __init__(self,\n",
    "                 dimension: Tuple[int, int],\n",
    "                 data: np.ndarray,  # 2d numpy array\n",
    "                 concept_indices: List[bool],  # boolean list if feature belongs to concept A or concept B\n",
    "                 num_layers: int = 100,\n",
    "                 layer_distance_ratio: float = 0.1,\n",
    "                 sigma: float = 1.0,\n",
    "                 learning_rate: float = 0.5,\n",
    "                 neighborhood_function: str = 'gaussian',\n",
    "                 activation_distance: str = 'euclidean',\n",
    "                 initial_codebook_inizialization: str = 'random',  # random or pretrained\n",
    "                 random_seed=None):\n",
    "        super().__init__(\n",
    "            x=dimension[0],\n",
    "            y=dimension[1],\n",
    "            input_len=data.shape[1],\n",
    "            sigma=sigma,\n",
    "            learning_rate=learning_rate,\n",
    "            neighborhood_function=neighborhood_function,\n",
    "            topology='rectangular',\n",
    "            activation_distance=activation_distance,\n",
    "            random_seed=random_seed)\n",
    "        self.data = data\n",
    "        self.dimension = dimension\n",
    "        self.concept_indices = concept_indices\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_distance_ratio = layer_distance_ratio\n",
    "        self._neighborhood_function = neighborhood_function\n",
    "        self._initial_codebook_inizialization = initial_codebook_inizialization\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.weights_by_layer: np.ndarray = self._create_weights_by_layer()\n",
    "        self.layers: List[Layer] = self._create_layers()\n",
    "        self.layer_distances = self._create_layer_distances()\n",
    "\n",
    "    def train(self,\n",
    "              data: np.ndarray,  # 2d numpy array,\n",
    "              num_iterations):\n",
    "        n_observations = data.shape[0]\n",
    "        for t in tqdm(range(num_iterations)):\n",
    "            selected_layer = randrange(0, self.num_layers)\n",
    "            selected_observation = randrange(0, n_observations)\n",
    "            # print(f'selected layer: {selected_layer}')\n",
    "            # print(f'selected observation: {selected_observation}')\n",
    "            winner = self.layers[selected_layer].winner(\n",
    "                data[selected_observation] * self.weights_by_layer[selected_layer])\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                # print(f'current layer: {i}')\n",
    "                # ĺayer_dist = self.layer_distance(t, num_iterations, np.abs(selected_layer - i))\n",
    "                ĺayer_dist = self.layer_distances[np.abs(selected_layer - i)]\n",
    "                # print(f'distance: {ĺayer_dist}')\n",
    "                layer.update(data[selected_observation] * self.weights_by_layer[i],\n",
    "                             winner,\n",
    "                             ĺayer_dist,\n",
    "                             t,\n",
    "                             num_iterations)\n",
    "\n",
    "    # the distance between one layer and the next is defined by the distance between neighboring units\n",
    "    # multiplyed by some fraction \"layer_distance_ratio\"\n",
    "    def layer_distance(self, t, max_iteration, grid_distance):\n",
    "        if grid_distance == 0.0:\n",
    "            return 1.0\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        distance_neighboring_units = self.neighborhood((0, 0), sig)[(0, 1)]\n",
    "        return distance_neighboring_units * (self.layer_distance_ratio / grid_distance)\n",
    "\n",
    "    # return the codebook weights for all layers\n",
    "    def get_layer_weights(self) -> List[np.ndarray]:\n",
    "        return [layer.get_weights() for layer in self.layers]\n",
    "    \n",
    "    def _create_layer_distances(self):\n",
    "        distance_matrix = self.neighborhood((0, 0), 1.0)\n",
    "        distance_neighboring_units = distance_matrix[(0, 1)]\n",
    "        distance_neighboring_units_fraction = distance_neighboring_units * self.layer_distance_ratio\n",
    "        distance_corner_units = distance_matrix[(self.dimension[0] - 1, self.dimension[1] - 1)]\n",
    "        #layer_distances = np.linspace(distance_neighboring_units_fraction, distance_corner_units, self.num_layers - 1)\n",
    "        layer_distances = dist\n",
    "        layer_distances = np.insert(layer_distances, 0, 1.0)  # distence to layer itself\n",
    "        return layer_distances\n",
    "\n",
    "    # create a weights matrix for two concepts in a feature matrix\n",
    "    # the shape corresponds to shape (num_layers, input_len))\n",
    "    # where num_soms is the number of soms trained\n",
    "    def _create_weights_by_layer(self):\n",
    "        if self.concept_indices.shape[0] != self._input_len:\n",
    "            raise AttributeError('concept_indices has to have the same dimension as input_len')\n",
    "        column_weights = []\n",
    "        weights_concept_1 = np.linspace(0, 1, self.num_layers)\n",
    "        weights_concept_2 = np.linspace(1, 0, self.num_layers)\n",
    "        for i in self.concept_indices:\n",
    "            if i:\n",
    "                column_weights.append(weights_concept_1)\n",
    "            else:\n",
    "                column_weights.append(weights_concept_2)\n",
    "        return np.column_stack(column_weights)\n",
    "\n",
    "    # initialize all layers of the aligned SOM\n",
    "    def _create_layers(self) -> List[Layer]:\n",
    "        layers = []\n",
    "        if self._initial_codebook_inizialization == 'random':\n",
    "            inital_weights = self._create_random_weights()\n",
    "        elif self._initial_codebook_inizialization == 'pretrained':\n",
    "            inital_weights = self._create_weights_by_training_one_some()\n",
    "        else:\n",
    "            raise AttributeError('initial_codebook_inizialization has to be \"random\" or \"pretrained\"')\n",
    "        for weights in self.weights_by_layer:\n",
    "            layers.append(Layer(\n",
    "                dimension=self.dimension,\n",
    "                input_len=self._input_len,\n",
    "                initial_codebook=np.array(inital_weights * weights, dtype=np.float32),\n",
    "                sigma=self._sigma,\n",
    "                learning_rate=self._learning_rate,\n",
    "                neighborhood_function=self._neighborhood_function,\n",
    "                activation_distance=self._activation_distance,\n",
    "                random_seed=self.random_seed))\n",
    "        return layers\n",
    "\n",
    "    def _create_random_weights(self):\n",
    "        if self.random_seed:\n",
    "            np.random.seed(self.random_seed)\n",
    "        return np.random.random((self.dimension[0], self.dimension[1], self._input_len))\n",
    "\n",
    "    def _create_weights_by_training_one_some(self):\n",
    "        # som trained on not weighted features (same as middle layer)\n",
    "        middle_som = MiniSom(\n",
    "            x=self.dimension[0],\n",
    "            y=self.dimension[1],\n",
    "            input_len=self._input_len,\n",
    "            sigma=self._sigma,\n",
    "            learning_rate=self._learning_rate,\n",
    "            neighborhood_function=self._neighborhood_function,\n",
    "            topology='rectangular',\n",
    "            activation_distance=self._activation_distance,\n",
    "            random_seed=self.random_seed)\n",
    "        middle_som.train(self.data, 1000)\n",
    "        return middle_som.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af949a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "N_LAYERS = 31\n",
    "SOM_DIM = (3, 4)\n",
    "TRAIN_STEPS = 1000\n",
    "\n",
    "input_data, components, weights, classinfo = load_dataset('animals')\n",
    "data = input_data['arr']\n",
    "concept_indices = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
    "concept_indices = np.abs(concept_indices - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "asom = AlignedSom(\n",
    "    SOM_DIM, data, concept_indices,\n",
    "    num_layers=N_LAYERS,\n",
    "    sigma=1.0,\n",
    "    initial_codebook_inizialization='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab041362",
   "metadata": {},
   "outputs": [],
   "source": [
    "asom.layer_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "asom.train(data, TRAIN_STEPS * N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c21baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDH - implementation\n",
    "def SDH(_m, _n, _weights, _idata, factor, approach):\n",
    "    import heapq\n",
    "\n",
    "    sdh_m = np.zeros( _m * _n)\n",
    "\n",
    "    cs=0\n",
    "    for i in range(factor): cs += factor-i\n",
    "\n",
    "    for vector in _idata:\n",
    "        dist = np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1))\n",
    "        c = heapq.nsmallest(factor, range(len(dist)), key=dist.__getitem__)\n",
    "        if (approach==0): # normalized\n",
    "            for j in range(factor):  sdh_m[c[j]] += (factor-j)/cs \n",
    "        if (approach==1):# based on distance\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0/dist[c[j]] \n",
    "        if (approach==2): \n",
    "            dmin, dmax = min(dist[c]), max(dist[c])\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0 - (dist[c[j]]-dmin)/(dmax-dmin)\n",
    "    \n",
    "    sdh_m = sdh_m.reshape(_m, _n)\n",
    "    return resize(sdh_m, (1000, 1000), mode='constant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_dimension = SOM_DIM\n",
    "visualizations = []\n",
    "for layer_weights in asom.get_layer_weights():\n",
    "    layer_weights = np.reshape(layer_weights, (som_dimension[0] * som_dimension[1], data.shape[1]))\n",
    "    # sdh = hv.Image(SDH(som_dimension[0], som_dimension[1], layer_weights, data, 2, 0)).opts(xaxis=None, yaxis=None)\n",
    "    sdh = SDH(som_dimension[0], som_dimension[1], layer_weights, data, 2, 2)\n",
    "    visualizations.append(sdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 5, figsize=(30,5))\n",
    "for i, vis_i in enumerate(np.linspace(0, N_LAYERS - 1, 5, dtype=int)):\n",
    "    sns.heatmap(visualizations[vis_i], ax=axis[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import panel as pn\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_dimension = SOM_DIM\n",
    "visualizations = []\n",
    "for layer_weights in asom.get_layer_weights():\n",
    "    layer_weights = np.reshape(layer_weights, (som_dimension[0] * som_dimension[1], data.shape[1]))\n",
    "    # sdh = hv.Image(SDH(som_dimension[0], som_dimension[1], layer_weights, data, 2, 0)).opts(xaxis=None, yaxis=None)\n",
    "    sdh = hv.Image(SDH(SOM_DIM[0], SOM_DIM[1], layer_weights, data, 2, 2)).opts(xaxis=None, yaxis=None)\n",
    "    visualizations.append(sdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Row(*visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd32235",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdh = hv.Image(SDH(SOM_DIM[0], SOM_DIM[1], w, data, 2, 0)).opts(xaxis=None, yaxis=None)\n",
    "sdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = asom.get_layer_weights()[0]\n",
    "w = np.reshape(w,(3*4,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "somoclu.Somoclu(SOM_DIM[0], SOM_DIM[1], initialcodebook=w).view_umatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c02a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySOMVis(weights_list=asom.get_layer_weights(), input_data=data)._mainview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabec01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed908ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations = []\n",
    "layer_weights = asom.get_layer_weights()\n",
    "for i in np.linspace(0, N_LAYERS - 1, 5, dtype=int):\n",
    "    visualizations.append(PySOMVis(weights=layer_weights[i], input_data=data)._mainview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6936d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pn.Row(*visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36267f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert True == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigx = np.arange(5)\n",
    "neigy = np.arange(5)\n",
    "neigz = np.arange(5) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = np.meshgrid(neigx, neigy, neigz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d11d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([a.reshape(1, -1)[0], b.reshape(1, -1)[0], c.reshape(1, -1)[0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf318429",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce706a05d73a5d38fe96da9c702e6e96f49768ddd8c7b5e7c74e8c1ba5bb9dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
